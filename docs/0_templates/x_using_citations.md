---
author: "Jorge Roldan"
date: '2025-03-02'
title: 'Using Citations'
tags: ['citations']
categories: ['basics']
ShowToc: true
ShowBreadCrumbs: true
draft: true
---
# Using a pretrained BERT model for sequence classification
## Model


This is the start of my citations ,[[^aiayn]][[^bert]]

Pipeline [^behind_the_pipeline]


<!-- [^good_fellow]: Goodfellow, Ian, et al. *Deep Learning*. MIT Press, 2016. -->

# References
[^bert]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds., Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171–4186. doi: 10.18653/v1/N19-1423.

[^aiayn]: A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023, arXiv: arXiv:1706.03762. Accessed: Apr. 03, 2024. [Online]. Available: http://arxiv.org/abs/1706.03762




[^behind_the_pipeline]: “Behind the pipeline.” [Online]. Available: https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt
